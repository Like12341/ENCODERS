/* Copyright (c) 2014 SRI International and Suns-tech Incorporated
 * Developed under DARPA contract N66001-11-C-4022.
 * Authors:
 *   Sam Wood (SW)
 *   Hasnain Lakhani (HL)
 */

#include "MemoryCache.h"

/*
 * An in-memory database based on hash tables for fast indexing. 
 *
 * NOTE: The repository funtions do not use an ID since we have no
 * notion of row id (this rowid was generated by the SQLDataStore).
 *
 * The "max_size" variables are used for compaction in compact().
 */

MemoryCache::MemoryCache() :
    excludeZeroWeightAttributes(false),
    enableCompaction(false),
    do_id_to_do_max_size(0),
    do_id_to_do_freed(0),
    do_id_to_do(new do_id_to_do_t()),
    node_id_to_node_max_size(0),
    node_id_to_node_freed(0),
    node_id_to_node(new node_id_to_node_t()),
    node_id_to_do_id_max_size(0),
    node_id_to_do_id_freed(0),
    node_id_to_do_id(new node_id_to_do_id_t()),
    iface_to_node_id_max_size(0),
    iface_to_node_id_freed(0),
    iface_to_node_id(new iface_to_node_id_t()),
    keyval_to_node_id_max_size(0),
    keyval_to_node_id_freed(0),
    keyval_to_node_id(new keyval_to_node_id_t()),
    key_star_to_node_ids_max_size(0),
    key_star_to_node_ids_freed(0),
    key_star_to_node_ids(new key_star_to_node_ids_t()),
    type_to_node_id_max_size(0),
    type_to_node_id_freed(0),
    type_to_node_id(new type_to_node_id_t()),
    keyval_to_do_id_max_size(0),
    keyval_to_do_id_freed(0),
    keyval_to_do_id(new keyval_to_do_id_t()),
    key_star_to_do_ids_max_size(0),
    key_star_to_do_ids_freed(0),
    key_star_to_do_ids(new key_star_to_do_ids_t()),
    filters_max_size(0),
    filters_freed(0),
    filters(new filters_t()),
    filter_id_to_attribute_max_size(0),
    filter_id_to_attribute_freed(0),
    filter_id_to_attribute(new filter_id_to_attribute_t()),
    keyval_to_filter_id_max_size(0),
    keyval_to_filter_id_freed(0),
    keyval_to_filter_id(new keyval_to_filter_id_t()),
    key_star_to_filter_id_max_size(0),
    key_star_to_filter_id_freed(0),
    key_star_to_filter_id(new key_star_to_filter_id_t()),
    availableFilterId(LONG_MAX),
    errorCount(0),
    maxNodesToMatch(0),
    authority_to_repository_max_size(0),
    authority_to_repository_freed(0),
    authority_to_repository(new authority_to_repository_t())
{
}

/*
 * Helper to construct a hash key from the attribute <key, val>
 */
static string
createHash(
    string key,
    string val)
{
    return key + string(":") + val;
}

/*
 * Helper to extract the attribute value from a hash key 
 */
static string
extractVal(string hash)
{
    return hash.substr(hash.find_last_of(":") + 1);
}

/*
 * Helper to extract the attribute key from a hash key
 */
static string
extractKey(string hash)
{
    return hash.substr(0, hash.find_last_of(":"));
}

/*
 * Get all the interface ids for a node. Caller should pass a list of strings
 * to be populated.
 */
void 
AbstractNode::getInterfaces(List<string> *interfaces)
{
    if (!interfaces) {
        HAGGLE_ERR("Got NULL interfaces\n");
        return;
    }

    if (!node && !fake_node) {
        HAGGLE_ERR("Abstract node is missing data\n");
        return;
    }

    if (!node) {
        for (List<string>::iterator it = fake_node->interfaces.begin(); it != fake_node->interfaces.end(); it++) {
            interfaces->push_back(*it);
        }
        return;
    }

    node.lock();
    const InterfaceRefList *lst = node->getInterfaces();
    for (InterfaceRefList::const_iterator it = lst->begin(); it != lst->end(); it++) {
        interfaces->push_back(Interface::idString(*it));
    }
    node.unlock();
}

/*
 * Get all the interests key=val:weight for a node. Caller should pass a List of
 * <string, int> tuples to be populated.
 */
void 
AbstractNode::getInterests(List<Pair <string, int> > *interests)
{
    if (!interests) {
        HAGGLE_ERR("Got NULL interests\n");
        return;
    }

    if (!node && !fake_node) {
        HAGGLE_ERR("Abstract node is missing data\n");
        return;
    }

    if (!node) {
        for (List<Pair <string, int> >::iterator it = fake_node->interests.begin(); it != fake_node->interests.end(); it++) {
            interests->push_back(make_pair((*it).first, (*it).second)); 
        }
        return;
    }

    node.lock();
    const Attributes *attrs = node->getAttributes();
    if (attrs) {
 	    for (Attributes::const_iterator it = attrs->begin(); 
	        it != attrs->end(); it++) {
		    const Attribute& a = (*it).second;
            string name = a.getName();
            string value = a.getValue();
            int weight = a.getWeight();
            string key = createHash(name, value);
            interests->push_back(make_pair(key, weight));
        }
    }
    node.unlock();
}

/*
 * Get all the attributes for the data object. Caller should pass an
 * allocated list of strings to be populated.
 */
void
AbstractDataObject::getAttributes(List<string> *new_attrs)
{

    if (!new_attrs) {
        HAGGLE_ERR("NULL new attrs\n");
        return;
    }

    if (!dObj) {
        for (List<string>::iterator it = attributes.begin(); it != attributes.end(); it++) {
            new_attrs->push_back(*it);
        }
        return;
    }
    
    const Attributes *attrs = dObj->getAttributes();
    if (attrs) {
        for (Attributes::const_iterator it = attrs->begin(); 
            it != attrs->end(); it++) {
            const Attribute& a = (*it).second;
            string name = a.getName();
            string value = a.getValue();
            string key = createHash(name, value);
            new_attrs->push_back(key);
        }
    }

    return;
}

/*
 * Delete the memory cache and free all the associated data structures.
 */
MemoryCache::~MemoryCache()
{
    for (do_id_to_do_t::iterator it = do_id_to_do->begin(); it != do_id_to_do->end(); it++) {
        AbstractDataObject *dobj = (*it).second;
        do_id_to_do->erase(it);
        delete dobj;
    }

    for (node_id_to_node_t::iterator it = node_id_to_node->begin(); it != node_id_to_node->end(); it++) {
        AbstractNode *node = (*it).second;
        node_id_to_node->erase(it);
        delete node;
    }

    for (node_id_to_do_id_t::iterator it = node_id_to_do_id->begin(); it != node_id_to_do_id->end(); it++) {
        node_id_to_do_id->erase(it);
    }

    for (iface_to_node_id_t::iterator it = iface_to_node_id->begin(); it != iface_to_node_id->end(); it++) {
        iface_to_node_id->erase(it);
    }

    for (keyval_to_node_id_t::iterator it = keyval_to_node_id->begin(); it != keyval_to_node_id->end(); it++) {
        keyval_to_node_id->erase(it);
    }

    for (keyval_to_do_id_t::iterator it = keyval_to_do_id->begin(); it != keyval_to_do_id->end(); it++) {
        keyval_to_do_id->erase(it);
    }

    for (key_star_to_do_ids_t::iterator it = key_star_to_do_ids->begin(); it != key_star_to_do_ids->end(); it++) {
        HashMap<string, string> *doids = (*it).second;
        if (!doids) {
            HAGGLE_ERR("Somehow have null doids in keystar for key: %s\n", (*it).first.c_str());
            errorCount++;
            continue;
        }
        for (HashMap<string, string>::iterator itt = doids->begin(); itt != doids->end(); itt++) {
            doids->erase(itt);
        }
        key_star_to_do_ids->erase(it);
        delete doids;
    } 

    for (key_star_to_node_ids_t::iterator it = key_star_to_node_ids->begin(); it != key_star_to_node_ids->end(); it++) {
        HashMap<string, int> *node_ids = (*it).second;
        if (!node_ids) {
            HAGGLE_ERR("Somehow have null node ids in keystar\n");
            errorCount++;
            continue;
        }
        for (HashMap<string, int>::iterator itt = node_ids->begin(); itt != node_ids->end(); itt++) {
            node_ids->erase(itt);
        }
        key_star_to_node_ids->erase(it);
        delete node_ids;
    }

    for (key_star_to_filter_id_t::iterator it = key_star_to_filter_id->begin(); it != key_star_to_filter_id->end(); it++) {
        HashMap<long, int> *filter_ids = (*it).second;
        if (!filter_ids) {
            HAGGLE_ERR("Somehow have null filter ids in keystar\n");
            errorCount++;
            continue;
        }
        for (HashMap<long, int>::iterator itt = filter_ids->begin(); itt != filter_ids->end(); itt++) {
            filter_ids->erase(itt);
        }
        key_star_to_filter_id->erase(it);
        delete filter_ids;
    }

    for (filter_id_to_attribute_t::iterator it = filter_id_to_attribute->begin(); it != filter_id_to_attribute->end(); it++) {
        filter_id_to_attribute->erase(it);
    }

    for (keyval_to_filter_id_t::iterator it = keyval_to_filter_id->begin(); it != keyval_to_filter_id->end(); it++) {
        keyval_to_filter_id->erase(it);
    }

    for (filters_t::iterator it = filters->begin(); it != filters->end(); it++) {
        filters->erase(it);
    }

    delete do_id_to_do;
    delete node_id_to_node;
    delete node_id_to_do_id;
    delete iface_to_node_id;
    delete keyval_to_node_id;
    delete key_star_to_node_ids;
    delete type_to_node_id;
    delete keyval_to_do_id;
    delete key_star_to_do_ids;
    delete filters;
    delete filter_id_to_attribute;
    delete keyval_to_filter_id;
    delete key_star_to_filter_id;
    delete authority_to_repository;
}

/*
 * Cache a data object.
 */
int
MemoryCache::cacheDataObject(
    DataObjectRef &dObj)
{
    return cacheDataObject(new AbstractDataObject(dObj));
}

/*
 * Cache a particular data object. Takes an abstract data object to
 * ease unit test writing. Returns 0 upon success, and -1 on failure.
 */
int
MemoryCache::cacheDataObject(
    AbstractDataObject *ad)
{
    if (!ad) {
        errorCount++;
        HAGGLE_ERR("NULL abstract data object\n");
        return -1;
    }
    string dobj_id = ad->getId();
    if (isDataObjectCached(dobj_id)) {
        HAGGLE_DBG2("Data object %s already cached, removing and re-inserting\n", dobj_id.c_str());
        unCacheDataObject(dobj_id);
    }
    do_id_to_do_max_size++;
    do_id_to_do->insert(make_pair(dobj_id, ad));

    List<string> attrs;
    ad->getAttributes(&attrs);

    for (List<string>::iterator it = attrs.begin(); it != attrs.end(); it++) {
        string keyval = (*it);
        string key = extractKey(keyval);
        string val = extractVal(keyval);

        keyval_to_do_id_max_size++;
        keyval_to_do_id->insert(make_pair(keyval, dobj_id));

        /*
         * Handle data object attributes with star "*"
         */

        HashMap<string, string> *doids;
        key_star_to_do_ids_t::iterator itt = key_star_to_do_ids->find(key);
        if (itt == key_star_to_do_ids->end()) {
            doids = new HashMap<string, string>();
            key_star_to_do_ids_max_size++;
            key_star_to_do_ids->insert(make_pair(key, doids));
        }
        else {
            doids = (*itt).second;
        }
        doids->insert(make_pair(dobj_id, val));
    }

    // support for node description data objects (in memory node descriptions
    // must be off)
    if (ad->getDataObject()) {
        // we don't know what order the node description / dobj inserts will be
        // so we place this code in both cacheDataObject and cacheNode
        DataObjectRef dobj = ad->getDataObject();
        if (dobj->isNodeDescription()) {
            NodeRef node = Node::create(dobj);
            string node_id = Node::idString(node);
            if (isNodeCached(node_id)) {
                node_id_to_do_id->insert(make_pair(node_id, dobj_id));
                node_id_to_do_id_max_size++;
            }
        }
        dobj->setStored();
    }

    return 0;
}

/*
 * Return true if and only if a data object with id `dobj_id` is cached.
 */
bool
MemoryCache::isDataObjectCached(
    const string &dobj_id)
{
    do_id_to_do_t::iterator it = do_id_to_do->find(dobj_id);
    if (it != do_id_to_do->end()) {
        return true;
    }
    return false;
}

/*
 * Get all the data objects from the database.
 */
DataObjectRefList
MemoryCache::getAllDataObjects()
{
    DataObjectRefList dobjs;
    for (do_id_to_do_t::iterator it = do_id_to_do->begin(); it != do_id_to_do->end(); it++) {
        DataObjectRef dObj = (*it).second->getDataObject();
        dobjs.add(dObj);
    }
    return dobjs;
}

/*
 * Get a data object from a particular identifier. 
 */
DataObjectRef
MemoryCache::getDataObjectFromId(
    const string &dobj_id)
{
    do_id_to_do_t::iterator it = do_id_to_do->find(dobj_id);
    if (it == do_id_to_do->end()) {
        HAGGLE_DBG2("Data object %s is missing\n", dobj_id.c_str());
        return NULL;
    }
    
    AbstractDataObject *ad = (*it).second;
    if (!ad) {
        HAGGLE_ERR("Somehow abstract dobj is NULL\n");
        errorCount++;
        return NULL;
    }
    DataObjectRef dObj = ad->getDataObject();
    // CBMEN, HL - DataObjects fresh from the database should not have this set.
    if (dObj->getIsForLocalApp()) {
        dObj->setIsForLocalApp(false);
    }
    return dObj;
}

/*
 * Remove a data object and its associated elements from the data structures.
 * structures.
 */
int
MemoryCache::unCacheDataObject(
    const string &dobj_id)
{
    AbstractDataObject *ad;
    {
        do_id_to_do_t::iterator it = do_id_to_do->find(dobj_id);
        if (it == do_id_to_do->end()) {
            HAGGLE_ERR("Cannot uncache missing data object: %s\n", dobj_id.c_str());
            return -1;
        }
        ad = (*it).second;
        do_id_to_do->erase(it);
    }

    List<string> attrs;
    ad->getAttributes(&attrs);

	for (List<string>::iterator it = attrs.begin(); it != attrs.end(); it++) {
        string key = (*it);
        string name = extractKey(key);
        string value = extractVal(key);
        for (keyval_to_do_id_t::iterator it = keyval_to_do_id->find(key); it != keyval_to_do_id->end(); it++) {
            if ((*it).first != key) {
                break;
            }
            if ((*it).second == dobj_id) {
                keyval_to_do_id->erase(it);
            }
        }

        // handle *:
        key_star_to_do_ids_t::iterator itt = key_star_to_do_ids->find(extractKey(key));
        if (itt == key_star_to_do_ids->end()) {
            continue;
        }
        HashMap<string, string> *doids = (*itt).second;
        for (HashMap<string, string>::iterator ittt = doids->find(dobj_id); ittt != doids->end(); ittt++) {
            if ((*ittt).first != dobj_id) {
                break;
            }
            doids->erase(ittt);
        }
        if (doids->size() == 0) {
            key_star_to_do_ids->erase(itt);
            delete doids;
        }
    }

    // support for node description data objects (in memory node descriptions
    // must be off)
    {
        DataObjectRef dobj = ad->getDataObject();
        if (dobj && dobj->isNodeDescription()) {
            NodeRef node = Node::create(dobj);
            string node_id = Node::idString(node);
            for (node_id_to_do_id_t::iterator it = node_id_to_do_id->find(node_id); it != node_id_to_do_id->end(); it++) {
                if ((*it).first != node_id) {
                    break;
                }
                if ((*it).second == dobj_id) {
                    node_id_to_do_id->erase(it);
                    break;
                }
            }
        }
    }

    delete ad;

    compact();

    return 0;
}

/*
 * Return `true` iff and a node is cached in the database.
 */
bool
MemoryCache::isNodeCached(
    const string &node_id)
{
    node_id_to_node_t::iterator it = node_id_to_node->find(node_id);
    if (it != node_id_to_node->end()) {
        return true;
    }
    return false;
}

/*
 * Insert a node into the database to be index for fast matching. 
 */
int
MemoryCache::cacheNode(
    NodeRef &node)
{
    return cacheNode(new AbstractNode(&node));
}

/*
 * Insert a node into the database to be indexed for fast matching. 
 * We use abstract nodes here to make unit testing easier.
 */
int
MemoryCache::cacheNode(
    AbstractNode *node)
{
    if (!node) {
        HAGGLE_ERR("NULL node\n");
        errorCount++;
        return -1;
    }
    string node_id = node->getId();
    if (isNodeCached(node_id)) {
        HAGGLE_DBG2("Node %s already cached, removing and re-inserting:\n", node_id.c_str());
        unCacheNode(node_id);
    }

    Node::Type_t type = node->getType();

    node_id_to_node_max_size++;
    node_id_to_node->insert(make_pair(node_id, node));

    {
        List<string> ifaces;
        node->getInterfaces(&ifaces);
        for (List<string>::iterator it = ifaces.begin(); it != ifaces.end(); it++) {
            string iface_str = (*it);
            iface_to_node_id_max_size++;
            iface_to_node_id->insert(make_pair(iface_str, node_id));
        }
    }
    
    {
        List<Pair< string, int> > interests;
        node->getInterests(&interests);
        for (List<Pair< string, int> >::iterator it = interests.begin(); it != interests.end(); it++) {
            string keyval = (*it).first;
            string key = extractKey(keyval);
            string val = extractVal(keyval);
            int weight = (*it).second;
            attribute_t attr;
            attr.keyval = keyval;
            attr.weight = weight;
            if (val != "*") {
                keyval_to_node_id_max_size++;
                keyval_to_node_id->insert(make_pair(keyval, node_id));
            }
            else {
                /*
                * Handle node interests with star "*"
                */
                key_star_to_node_ids_t::iterator it = key_star_to_node_ids->find(key);
                HashMap<string, int> *node_ids;
                if (it == key_star_to_node_ids->end()) {
                    node_ids = new HashMap<string, int>();
                    key_star_to_node_ids_max_size++;
                    key_star_to_node_ids->insert(make_pair(key, node_ids));
                }
                else {
                    node_ids  = (*it).second;
                }
                // we use a bogus value for int
                node_ids->insert(make_pair(node_id, 1));
            }
        }
    }

    type_to_node_id_max_size++;
    type_to_node_id->insert(make_pair(type, node_id));

    // support for node description data objects (in memory node descriptions
    // must be off)
    if (node->getDataObject()) {
        // we don't know what order the node description / dobj inserts will be
        // so we place this code in both cacheDataObject and cacheNode
        DataObjectRef dobj = node->getDataObject();
        string dobj_id = DataObject::idString(dobj);
        if (isDataObjectCached(dobj_id)) {
            node_id_to_do_id->insert(make_pair(node_id, dobj_id));
            node_id_to_do_id_max_size++;
        }
    }
    
    
    return 0;
}

/*
 * Retrieve all of the nodes from the database. Useful for dumping the database
 * to disk.
 */
NodeRefList
MemoryCache::getAllNodes()
{
    NodeRefList nodes;

    for (node_id_to_node_t::iterator it = node_id_to_node->begin(); it != node_id_to_node->end(); it++) {
        nodes.push_back((*it).second->getNode());
    }

    return nodes;
}

/*
 * Get all the nodes with a particular type `type`.
 */
NodeRefList
MemoryCache::getNodesFromType(Node::Type_t type)
{
    NodeRefList nodes;
    for (type_to_node_id_t::iterator it = type_to_node_id->find(type); it != type_to_node_id->end(); it++) {
        if ((*it).first != type) {
            break;
        }
        string node_id = (*it).second;
        NodeRef node = getNodeFromId(node_id);
        if (!node) {
            HAGGLE_ERR("Could not get node for id: %s\n", node_id.c_str());
            errorCount++;
            continue;
        }
        nodes.push_back(node);
    }
    return nodes;
}

/*
 * Get a node from its ids.
 */
NodeRef
MemoryCache::getNodeFromId(
    const string &node_id)
{
    if (!isNodeCached(node_id)) {
        HAGGLE_DBG2("Node %s is missing\n", node_id.c_str());
        return NULL;
    }
    
    node_id_to_node_t::iterator it = node_id_to_node->find(node_id);
    return (*it).second->getNode();
}

/*
 * get a node from an interface id.
 */
NodeRef
MemoryCache::getNodeFromIfaceId(
    const string &iface_id)
{
    iface_to_node_id_t::iterator itt = iface_to_node_id->find(iface_id);
    if (itt == iface_to_node_id->end()) {
        HAGGLE_DBG2("Interface is missing %s\n", iface_id.c_str());
        return NULL;
    }

    string node_id = (*itt).second; 
    if (!isNodeCached(node_id)) {
        HAGGLE_ERR("Node %s is missing for interface %s\n", node_id.c_str(), iface_id.c_str());
        errorCount++;
        return NULL;
    }

    return getNodeFromId(node_id);
}

/*
 * Remove a node and it's associated elements from all the data structures.
 */
int
MemoryCache::unCacheNode(
    const string &node_id)
{
    AbstractNode *node;
    {
        node_id_to_node_t::iterator it = node_id_to_node->find(node_id);
        if (it == node_id_to_node->end()) {
            HAGGLE_ERR("Cannot uncache missing node: %s\n", node_id.c_str());
            return -1;
        }
        node = (*it).second;
        node_id_to_node->erase(it);
    }

    {
        List<string> ifaces;
        node->getInterfaces(&ifaces);
        for (List<string>::iterator it = ifaces.begin(); it != ifaces.end(); it++) {
            string iface_str = (*it);
            iface_to_node_id_t::iterator itt = iface_to_node_id->find(iface_str);
            if (itt == iface_to_node_id->end()) {
                HAGGLE_ERR("Interface %s missing for node %s\n", iface_str.c_str(), node_id.c_str());
                errorCount++;
                continue;
            }
            iface_to_node_id->erase(itt);
        }
    }

    {
        List<Pair< string, int> > interests;
        node->getInterests(&interests);
        for (List<Pair< string, int> >::iterator it = interests.begin(); it != interests.end(); it++) {
            string keyval = (*it).first;
            string key = extractKey(keyval);
            string val = extractVal(keyval);

            if (val != "*") {
                for (keyval_to_node_id_t::iterator it = keyval_to_node_id->find(keyval); it != keyval_to_node_id->end(); it++) {
                    if ((*it).first != keyval) {
                        break;
                    }
                    if ((*it).second == node_id) {
                        keyval_to_node_id->erase(it);
                    }
                }
            }
            else {
                key_star_to_node_ids_t::iterator it = key_star_to_node_ids->find(key);
                if (it == key_star_to_node_ids->end()) {
                    HAGGLE_ERR("Keystar missing for node\n");
                    errorCount++;
                    continue;
                }
                HashMap<string, int> *node_ids = (*it).second;
                if (!node_ids) {
                    HAGGLE_ERR("Keystar list is NULL\n");
                    errorCount++;
                    continue;
                }
                HashMap<string, int>::iterator itt = node_ids->find(node_id);
                if (itt == node_ids->end()) {
                    HAGGLE_ERR("Node missing from key star list\n");
                    errorCount++;
                    continue;
                }
                node_ids->erase(itt);
                if (node_ids->size() == 0) {
                    key_star_to_node_ids->erase(it);
                    delete node_ids;
                }
            }
        }
    }

    bool foundEntry = false;
    Node::Type_t type = node->getType();
    for (type_to_node_id_t::iterator it = type_to_node_id->find(type); it != type_to_node_id->end(); it++) {
        if ((*it).first != type) {
            break;
        }
        if ((*it).second == node_id) {
            foundEntry = true;
            type_to_node_id->erase(it);
        }
    }

    if (!foundEntry) {
        HAGGLE_ERR("Type to node id index is mismatched: %s\n", node_id.c_str());
        errorCount++;
    }

    // support for node description data objects (in memory node descriptions
    // must be off)
    {
        for (node_id_to_do_id_t::iterator it = node_id_to_do_id->find(node_id); it != node_id_to_do_id->end(); it++) {
            if ((*it).first != node_id) {
                break;
            }
            node_id_to_do_id->erase(it);
        }
    }

    delete node;

    compact();
    
    return 0;
}

/*
 * Remove a filter and its associated elements from all the data structures.
 */
int
MemoryCache::unCacheFilter(
    long filter_id)
{
    for (filter_id_to_attribute_t::iterator it = filter_id_to_attribute->find(filter_id); it != filter_id_to_attribute->end(); it++) {
        if ((*it).first != filter_id) {
            break;
        }
        attribute_t a = (*it).second;
        string keyval = a.keyval;
        string key = extractKey(keyval);
        string val = extractVal(keyval);
        filter_id_to_attribute->erase(it);
        if (val != "*") {
            for (keyval_to_filter_id_t::iterator it = keyval_to_filter_id->find(keyval); it != keyval_to_filter_id->end(); it++) {
                if ((*it).first != keyval) {
                    break;
                }
                if ((*it).second == filter_id) {
                    keyval_to_filter_id->erase(it);
                    break;
                }
            }
        }
        else {
            key_star_to_filter_id_t::iterator it = key_star_to_filter_id->find(key);
            if (it == key_star_to_filter_id->end()) {
                HAGGLE_ERR("Keystar missing for filter\n");
                errorCount++;
                continue;
            }
            HashMap<long, int> *filter_ids = (*it).second;
            if (!filter_ids) {
                HAGGLE_ERR("Keystar list is NULL\n");
                errorCount++;
                continue;
            }
            HashMap<long, int>::iterator itt = filter_ids->find(filter_id);
            if (itt == filter_ids->end()) {
                HAGGLE_ERR("Filter id missing from key star list\n");
                errorCount++;
                continue;
            }
            filter_ids->erase(itt);
            if (filter_ids->size() == 0) {
                key_star_to_filter_id->erase(it);
                delete filter_ids;
            }
        }
    }
    filters_t::iterator it = filters->find(filter_id);
    if (it != filters->end()) {
        filters->erase(it);
    }

    compact();

    return 0;
}

/*
 * Set the parameters on the filter to specify the matching threshold and
 * the maximum number of matches. NOTE: this functionality was not in the
 * original haggle.
 */
void
MemoryCache::setFilterSettings(
    long filter_id,
    long matchThreshold,
    long maxMatches)
{
    filters_t::iterator it = filters->find(filter_id);
    if (it != filters->end()) {
        filters->erase(it);
    }
    filter_t f;
    f.matchThreshold = matchThreshold;
    f.maxMatches = maxMatches;
    filters_max_size++;
    filters->insert(make_pair(filter_id, f));
}

/*
 * Check if a filter with id `filter_id` is in the database.
 */
bool
MemoryCache::isFilterCached(
    long filter_id)
{
    filter_id_to_attribute_t::iterator it = filter_id_to_attribute->find(filter_id);
    if (it == filter_id_to_attribute->end()) {
        return false;
    }
    return true;
}

/*
 * Insert a filter into the data base. If `filter_id` does not belong to an
 * existing filter, then a new filter entry will be created in the data
 * structures. Otherwise, the key=val:weight parameters will be added
 * to the filter.
 */
int
MemoryCache::cacheFilter(
    long filter_id,
    string key,
    string val,
    int weight)
{
    if (filter_id == availableFilterId) {
        HAGGLE_ERR("Filter id collision!\n");
        return -1;
    }
    string keyval = createHash(key, val);
    attribute_t a;
    a.keyval = keyval;
    a.weight = weight;
    filter_id_to_attribute_max_size++;
    filter_id_to_attribute->insert(make_pair(filter_id, a));

    if (val != "*") {
        keyval_to_filter_id_max_size++;
        keyval_to_filter_id->insert(make_pair(keyval, filter_id));
    }
    else {
        key_star_to_filter_id_t::iterator it = key_star_to_filter_id->find(key);
        HashMap<long, int> *filter_ids; 
        if (it == key_star_to_filter_id->end()) {
            filter_ids = new HashMap<long, int>();
            key_star_to_filter_id_max_size++;
            key_star_to_filter_id->insert(make_pair(key, filter_ids));
        }
        else {
            filter_ids = (*it).second;
        }
        // second value is bogus
        filter_ids->insert(make_pair(filter_id, 1));
    }

    filters_t::iterator it = filters->find(filter_id);
    if (it == filters->end()) {
    	filter_t f;
    	f.matchThreshold = 100;
    	f.maxMatches = 0;
        filters_max_size++;
    	filters->insert(make_pair(filter_id, f));
    }

    return 0;
}

/*
 * Helper function to insert a data object id and metric into a sorted list.
 */
static
void sortedStringListInsert(
    List<Pair<string, double> >& list, 
    string dobj_id,
    double metric)
{
    List<Pair<string, double> >::iterator it = list.begin();
	
    for (; it != list.end(); it++) {
        if (metric > (*it).second) {
            break;
        }
    }
    list.insert(it, make_pair(dobj_id, metric));
}

/*
 * Get a list of all the filter ids that match a particular data object id. 
 */
List<long>
MemoryCache::getFiltersForDataObjectId(
    string dobj_id)
{
    Map<long, bool> eventmap;
    List<long> filter_ids;

    AbstractDataObject *dobj;
    {
        do_id_to_do_t::iterator it = do_id_to_do->find(dobj_id);
        if (it == do_id_to_do->end()) {
            HAGGLE_ERR("Data object is not cached\n");
            errorCount++;
            return filter_ids;
        }
        dobj = (*it).second;
    }

    List<string> attrs;
    dobj->getAttributes(&attrs);

    for (List<string>::iterator it = attrs.begin(); it != attrs.end(); it++) {
        string keyval = (*it);
        string key = extractKey(keyval);
        string val = extractVal(keyval);

        for (HashMap<string, long>::iterator it = keyval_to_filter_id->find(keyval); it != keyval_to_filter_id->end(); it++) {
            if ((*it).first != keyval) {
                break;
            }
            long filter_id = (*it).second;
            filters_t::iterator itt = filters->find(filter_id);
            if (itt == filters->end()) {
                HAGGLE_ERR("Filter %ld is missing from cache, dobj: %s\n", filter_id, dobj_id.c_str());
                continue;
            }
            filter_t f = (*itt).second;
            long matchThreshold = f.matchThreshold;
            int attr_matched; // we ignore it for this lookup
            double sat = computeFilterSatisfaction(filter_id, dobj_id, &attr_matched);
            Map<long, bool>::iterator ittt = eventmap.find(filter_id);
            bool isInList = ittt != eventmap.end();
            if (!isInList && sat*100 >= matchThreshold) {
                eventmap.insert(make_pair(filter_id, true));
                filter_ids.push_back(filter_id);
            }
        }

        key_star_to_filter_id_t::iterator itt = key_star_to_filter_id->find(key);

        if (itt == key_star_to_filter_id->end()) {
            continue;
        }

        HashMap<long, int> *key_star_to_filter_id = (*itt).second;
        if (!key_star_to_filter_id) {
            HAGGLE_ERR("NULL key star to filter ids entry\n");
            continue;
        }

        for (HashMap<long, int>::iterator it = key_star_to_filter_id->begin(); it != key_star_to_filter_id->end(); it++) {
            long filter_id = (*it).first;
            filters_t::iterator itt = filters->find(filter_id);
            if (itt == filters->end()) {
                HAGGLE_ERR("Filter %ld is missing from cache, dobj: %s\n", filter_id, dobj_id.c_str());
                continue;
            }
            filter_t f = (*itt).second;
            long matchThreshold = f.matchThreshold;
            int attr_matched; // we ignore it for this lookup
            double sat = computeFilterSatisfaction(filter_id, dobj_id, &attr_matched);
            Map<long, bool>::iterator ittt = eventmap.find(filter_id);
            bool isInList = ittt != eventmap.end();
            if (!isInList && sat*100 >= matchThreshold) {
                eventmap.insert(make_pair(filter_id, true));
                filter_ids.push_back(filter_id);
            }
        }
    }

    return filter_ids;
}

/*
 * Get a list of all the matching data object ids for a particular filter
 * id. Helper function to getDataObjectsForFilter().
 * NOTE: if max matches is set then we return the top max matches.
 */
List<string>
MemoryCache::getDataObjectIdsForFilter(
    long filter_id)
{
    filters_t::iterator it = filters->find(filter_id);
    List<string> matchListDataObjects;

    if (it == filters->end()) {
        HAGGLE_ERR("Missing filter options\n");
        return matchListDataObjects;
    }
    filter_t f = (*it).second;

    int num_matched = 0;
    long matchThreshold = f.matchThreshold;
    long maxMatch = f.maxMatches;

    bool enable_max_matches = maxMatch > 0;

    Map<string, bool> inList;
    List<Pair<string, double> > matchList;

    for (filter_id_to_attribute_t::iterator it = filter_id_to_attribute->find(filter_id); it != filter_id_to_attribute->end(); it++) {
        if ((*it).first != filter_id) {
            break;
        }
        string keyval = (*it).second.keyval;
        string key = extractKey(keyval);
        string val = extractVal(keyval);
        int weight = (*it).second.weight;

        if (excludeZeroWeightAttributes && weight == 0) {
            // treat zero weight attributes as if they're not in the DB
            continue;
        }

        if (val != "*") {
            // search for matching fully defined key=val attributes
            for (keyval_to_do_id_t::iterator it = keyval_to_do_id->find(keyval); it != keyval_to_do_id->end(); it++) {
                if ((*it).first != keyval) {
                    break;
                }
                string dobj_id = (*it).second;
                int attr_matched;
                double sat = computeFilterSatisfaction(filter_id, dobj_id, &attr_matched);
                Map<string, bool>::iterator itt = inList.find(dobj_id);
                bool isInList = (itt != inList.end());
                if (!isInList && sat*100 >= matchThreshold) {
                    sortedStringListInsert(matchList, dobj_id, sat);
                    inList.insert(make_pair(dobj_id, true));
                    num_matched++;
                }
            }
        }
        else {
            // search for matching key=* wildcard attributes
            key_star_to_do_ids_t::iterator it = key_star_to_do_ids->find(key);
            if (it == key_star_to_do_ids->end()) {
                continue;
            }

            HashMap<string, string> *key_star_to_do_ids = (*it).second;
            if (!key_star_to_do_ids) {
                HAGGLE_ERR("NULL key star to do id entry\n");
                continue;
            }

            for (HashMap<string, string>::iterator it = key_star_to_do_ids->begin(); it != key_star_to_do_ids->end(); it++) {
                string dobj_id = (*it).first;
                int attr_matched;
                double sat = computeFilterSatisfaction(filter_id, dobj_id, &attr_matched);
                Map<string, bool>::iterator itt = inList.find(dobj_id);
                bool isInList = (itt != inList.end());
                if (!isInList && sat*100 >= matchThreshold) {
                    sortedStringListInsert(matchList, dobj_id, sat);
                    inList.insert(make_pair(dobj_id, true));
                    num_matched++;
                }
            }
        }
    }

    for (List<Pair<string, double> >::iterator it = matchList.begin(); it != matchList.end() && (!enable_max_matches || (int) matchListDataObjects.size() < maxMatch); it++) {
        string dobj_id = (*it).first;
        matchListDataObjects.push_back(dobj_id);
    }

    return matchListDataObjects;
}

/*
 * Get all the data objects that match a particular filter id. 
 */
DataObjectRefList
MemoryCache::getDataObjectsForFilter(
    long filter_id)
{
    List<string> matchListDataObjects = getDataObjectIdsForFilter(filter_id);
    DataObjectRefList matched;

    for (List<string>::iterator it = matchListDataObjects.begin(); it != matchListDataObjects.end(); it++) {
        string dobj_id = (*it);
        DataObjectRef dObj = getDataObjectFromId(dobj_id);
        if (!dObj) {
            HAGGLE_ERR("Data object is missing from cache: %s\n", dobj_id.c_str());
            continue;
        }
        matched.add(dObj);
    }

    return matched;
}

/*
 * Compute the degree of satisfaction between a filter `filter_id` and data
 * object `dobj_id`. `o_attr_matched` returns the number of matching attributes.
 */
double
MemoryCache::computeFilterSatisfaction(
    long filter_id,
    const string &dobj_id,
    int *o_attr_matched)   
{
    int total = 0;
    int sum = 0;
    int match_count = 0;
    int attr_count = 0;
    for (filter_id_to_attribute_t::iterator it = filter_id_to_attribute->find(filter_id); it != filter_id_to_attribute->end(); it++) {
        if ((*it).first != filter_id) {
            break;
        }
        attr_count++;
        attribute_t a = (*it).second;
        string keyval = a.keyval;
        string key = extractKey(keyval);
        string val = extractVal(keyval);
        bool has_attr = false;
        int weight = a.weight;
        total += weight;
        if (val != "*") {
            // first check fully defined key=val attributes
            for (keyval_to_do_id_t::iterator it = keyval_to_do_id->find(keyval); it != keyval_to_do_id->end(); it++) {
                if ((*it).first != keyval) {
                    break;
                }
                if (dobj_id == (*it).second) {
                    has_attr = true;
                    break;
                }
            }
        }
        else {
            do {
                // check wildcard attributes
                key_star_to_do_ids_t::iterator it = key_star_to_do_ids->find(key);
                if (it == key_star_to_do_ids->end()) {
                    break;
                }
                HashMap<string, string> *key_star_to_do_ids = (*it).second;
                if (!key_star_to_do_ids) {
                    HAGGLE_ERR("NULL key star to do id entry\n");
                    break;
                }

                HashMap<string, string>::iterator itt = key_star_to_do_ids->find(dobj_id);
                if (itt != key_star_to_do_ids->end()) {
                    has_attr = true;
                    break;
                }
            } while (false);
        }

        if (has_attr) {
            sum += weight;
            match_count++;
        }
    }
    if (o_attr_matched) {
        *o_attr_matched = match_count;
    }
    if (total <= 0) {
    }
    return sum/(double)total;
}

/*
 * Compute the degree of satisfaction (aka matching) between a particular
 * node with id `node_id` and data object with id `dobj_id`.
 * `o_attr_matched` returns the number of attributes that matched.
 */
double
MemoryCache::computeSatisfaction(
    const string &node_id,
    const string &dobj_id,
    int *o_attr_matched)
{
    int total = 0;
    int sum = 0;
    int match_count = 0;

    AbstractNode *node;
    {
        node_id_to_node_t::iterator it = node_id_to_node->find(node_id);
        if (it == node_id_to_node->end()) {
            HAGGLE_ERR("Node %s is not cached\n", node_id.c_str());
            errorCount++;
            return 0;
        }
        node = (*it).second;
    }

    List<Pair<string, int> > interests;
    node->getInterests(&interests);
    
    for (List<Pair<string, int> >::iterator it = interests.begin(); it != interests.end(); it++) {
        string keyval = (*it).first;
        string key = extractKey(keyval);
        string val = extractVal(keyval);
        bool has_attr = false;
        int weight = (*it).second;
        if (weight == 0 && excludeZeroWeightAttributes) {
            // treat zero weight attributes as if they're not in the DB
            continue;
        }
        total += weight;
        if (val != "*") {
            for (keyval_to_do_id_t::iterator it = keyval_to_do_id->find(keyval); it != keyval_to_do_id->end(); it++) {
                if ((*it).first != keyval) {
                    break;
                }
                if (dobj_id == (*it).second) {
                    has_attr = true;
                    break;
                }
            }
        }
        else {
            do {
                key_star_to_do_ids_t::iterator it = key_star_to_do_ids->find(key);
                if (it == key_star_to_do_ids->end()) {
                    break;
                }
                HashMap<string, string> *key_star_to_do_ids = (*it).second;
                if (!key_star_to_do_ids) {
                    HAGGLE_ERR("NULL key star to do id entry\n");
                    break;
                }

                HashMap<string, string>::iterator itt = key_star_to_do_ids->find(dobj_id);
                if (itt != key_star_to_do_ids->end()) {
                    has_attr = true;
                    break;
                }
            } while (false);
        }

        if (has_attr) {
            sum += weight;
            match_count++;
        }
    }
    if (o_attr_matched) {
        *o_attr_matched = match_count;
    }
    if (total <= 0) {
        return 0;
    }
    return sum/(double)total;
}

/*
 * Return all the data object ids for a particular node id. This function
 * is only useful if in memory node descriptions are disabled.
 */
List<string>
MemoryCache::getNodeDataObjectIdsForNode(string &node_id)
{
    List<string> dobjs;
    for (node_id_to_do_id_t::iterator it = node_id_to_do_id->find(node_id); it != node_id_to_do_id->end(); it++) {
        if ((*it).first != node_id) {
            break;
        }
        dobjs.push_back((*it).second);
    }
    return dobjs;
}

/*
 * Return all the data object for a particular node id. This function
 * is only useful if in memory node descriptions are disabled.
 */
DataObjectRefList
MemoryCache::getNodeDataObjectsForNode(string &node_id)
{
    List<string> nodeDataObjects = getNodeDataObjectIdsForNode(node_id);
    DataObjectRefList dobjs;

    for (List<string>::iterator it = nodeDataObjects.begin(); it != nodeDataObjects.end(); it++) {
        string dobj_id = (*it);
        DataObjectRef dObj = getDataObjectFromId(dobj_id);
        if (!dObj) {
            HAGGLE_ERR("Data object is missing from cache: %s\n", dobj_id.c_str());
            continue;
        }
        dobjs.add(dObj);
    }

    return dobjs;
}

/*
 * Return all the data object Ids for a particular node. 
 * Helper function for getDataObjectsForNode()
 */
List<string>
MemoryCache::getDataObjectIdsForNode(
    const string &node_id,
    int min_attr_match,
    int max_matches,
    int threshold)
{

    List<string> matchListDataObjects;

    AbstractNode *node;
    {
        node_id_to_node_t::iterator it = node_id_to_node->find(node_id);
        if (it == node_id_to_node->end()) {
            HAGGLE_ERR("Node %s is not cached\n", node_id.c_str());
            errorCount++;
            return matchListDataObjects;
        }
        node = (*it).second;
    }

    long matchThreshold = node->getMatchThreshold();
    long maxMatch = node->getMaxDataObjectsInMatch();

    bool enable_max_matches = true;
    int num_matched = 0;

    if (max_matches > 0) {
        maxMatch = max_matches;
    }

    if (max_matches == 0) {
        enable_max_matches = false;
    }

    if (threshold >= 0) {
        matchThreshold = threshold;
    }

    Map<string, bool> inList;
    List<Pair<string, double> > matchList;

    List<Pair<string, int> > interests;
    node->getInterests(&interests);

    // iterate across interests checking for data objects w/ them as attributes
    for (List<Pair<string, int> >::iterator it = interests.begin(); it != interests.end(); it++) {
        string keyval = (*it).first;
        string key = extractKey(keyval);
        string val = extractVal(keyval);
        int weight = (*it).second;

        if (excludeZeroWeightAttributes && weight == 0) {
            // treat zero weight attributes as if they're not in the DB
            continue;
        }

        if (val != "*") {
            // handle non wildcard interests
            for (keyval_to_do_id_t::iterator it = keyval_to_do_id->find(keyval); it != keyval_to_do_id->end(); it++) {
                if ((*it).first != keyval) {
                    break;
                }
                string dobj_id = (*it).second;
                int attr_matched;
                double sat = computeSatisfaction(node_id, dobj_id, &attr_matched); 
                Map<string, bool>::iterator itt = inList.find(dobj_id);
                bool isInList = (itt != inList.end());
                if (!isInList && attr_matched >= min_attr_match && sat*100 >= matchThreshold) {
                    sortedStringListInsert(matchList, dobj_id, sat);
                    inList.insert(make_pair(dobj_id, true));
                    num_matched++;
                }
            }
        }
        else {
            // handle wild card interests
            key_star_to_do_ids_t::iterator it = key_star_to_do_ids->find(key);
            if (it == key_star_to_do_ids->end()) {
                continue;
            }

            HashMap<string, string> *key_star_to_do_ids = (*it).second;
            if (!key_star_to_do_ids) {
                HAGGLE_ERR("NULL key star to do id entry\n");
                continue;
            }

            for (HashMap<string, string>::iterator it = key_star_to_do_ids->begin(); it != key_star_to_do_ids->end(); it++) {
                string dobj_id = (*it).first;
                int attr_matched;
                double sat = computeSatisfaction(node_id, dobj_id, &attr_matched); 
                Map<string, bool>::iterator itt = inList.find(dobj_id);
                bool isInList = (itt != inList.end());
                if (!isInList && attr_matched >= min_attr_match && sat*100 >= matchThreshold) {
                    sortedStringListInsert(matchList, dobj_id, sat);
                    inList.insert(make_pair(dobj_id, true));
                    num_matched++;
                }
            }
        }
    }

    for (List<Pair<string, double> >::iterator it = matchList.begin(); it != matchList.end() && (!enable_max_matches || (int) matchListDataObjects.size() < maxMatch); it++) {
        string dobj_id = (*it).first;
        matchListDataObjects.push_back(dobj_id);
    }

    return matchListDataObjects;
}


/*
 * Return all the data objects that a particular node is interested in. 
 * `min_attr_match` is the minimum number of attributes to match
 * `max_matches` is the maximum number of data objects to match. 
 *      if < 0 then it will default to the node's default
 * `threshold` is the minimum matching threshold.
 *      if < 0 then it will default to the node's default
 */
DataObjectRefList
MemoryCache::getDataObjectsForNode(
    NodeRef &node,
    int min_attr_match,
    int max_matches,
    int threshold)
{

    string node_id = Node::idString(node);
    List<string> matchListDataObjects = getDataObjectIdsForNode(node_id, min_attr_match, max_matches, threshold);
    DataObjectRefList matched;

    for (List<string>::iterator it = matchListDataObjects.begin(); it != matchListDataObjects.end(); it++) {
        string dobj_id = (*it);
        DataObjectRef dObj = getDataObjectFromId(dobj_id);
        if (!dObj) {
            HAGGLE_ERR("Data object is missing from cache: %s\n", dobj_id.c_str());
            continue;
        }
        matched.add(dObj);
    }

    return matched;
}

/*
 * Get `max_matches` node ids that are interested in a particular data object,
 * with at least `min_attr_match` matching attributes.
 */
List<string>
MemoryCache::getNodeIdsForDataObject(
    const string &dobj_id,
    int min_attr_match,
    int max_matches)
{
    bool enable_max_matches = true;
    int num_matched = 0;

    if (max_matches < 0) {
        max_matches = maxNodesToMatch;
    }

    enable_max_matches = max_matches > 0;

    List<Pair<string, double> > matchList;
    Map<string, bool> inList;
    List<string> node_ids;

    // find the original data object from the base
    AbstractDataObject *dobj;
    {
        do_id_to_do_t::iterator it = do_id_to_do->find(dobj_id);
        if (it == do_id_to_do->end()) {
            HAGGLE_ERR("Data object %s is not cached\n", dobj_id.c_str());
            errorCount++;
            return node_ids;
        }
        dobj = (*it).second;
    }
    
    List<string> attrs;
    dobj->getAttributes(&attrs);

    // iterate across all the attributes, and use them as keys into the 
    // key_val -> node_id index if the matching degree is sufficient.
    for (List<string>::iterator it = attrs.begin(); it != attrs.end(); it++) {
        string keyval = (*it);
        string key = extractKey(keyval);
        string val = extractVal(keyval);

        for (keyval_to_node_id_t::iterator it = keyval_to_node_id->find(keyval); it != keyval_to_node_id->end(); it++) {
            string node_id = (*it).second;
            if ((*it).first != keyval) {
                break;
            }

            node_id_to_node_t::iterator itx = node_id_to_node->find(node_id);
            if (itx == node_id_to_node->end()) {
                HAGGLE_ERR("Node %s is missing from cache\n", node_id.c_str());
                errorCount++;
                continue;
            }
            AbstractNode *node = (*itx).second;
            if (!node) {
                HAGGLE_ERR("NULL node.\n");
                continue;
            }
            if (excludeZeroWeightAttributes) {
                // do not include matches w/ zero weight !
                List<Pair <string, int> > interests;
                node->getInterests(&interests);
                bool skip_interest = false;
                for (List<Pair <string, int> >::iterator ity = interests.begin(); ity != interests.end(); ity++) {
                    string node_keyval = (*ity).first;
                    int node_weight = (*ity).second;
                    if (keyval == node_keyval  && node_weight == 0) {
                        skip_interest = true;
                        break;  // get out of the interest for loop
                    }
                }
                if (skip_interest) {
                    // skip from matching all together
                    continue;
                }
            }
            long matchThreshold = node->getMatchThreshold();
            int attr_matched;
            double sat = computeSatisfaction(node_id, dobj_id, &attr_matched);
            Map<string, bool>::iterator itt = inList.find(node_id);
            bool isInList = (itt != inList.end());
            if (!isInList && attr_matched >= min_attr_match && sat*100 >= matchThreshold) {
                inList.insert(make_pair(node_id, true));
                sortedStringListInsert(matchList, node_id, sat);
                num_matched++;
            }
        }

        /*
         * Handle node interests with star "*"
         */

        key_star_to_node_ids_t::iterator itt = key_star_to_node_ids->find(key);
        if (itt == key_star_to_node_ids->end()) {
            continue;
        }

        HashMap<string, int> *key_star_to_node_ids = (*itt).second;
        if (!key_star_to_node_ids) {
            HAGGLE_ERR("NULL key star to node ids entry\n");
            continue;
        }

        
        for (HashMap<string, int>::iterator it = key_star_to_node_ids->begin(); it != key_star_to_node_ids->end(); it++) {
            string node_id = (*it).first;

            node_id_to_node_t::iterator itz = node_id_to_node->find(node_id);
            if (itz == node_id_to_node->end()) {
                HAGGLE_ERR("Node %s is missing from cache\n", node_id.c_str());
                errorCount++;
                continue;
            }
            AbstractNode *node = (*itz).second;
            long matchThreshold = node->getMatchThreshold();
            int attr_matched;
            double sat = computeSatisfaction(node_id, dobj_id, &attr_matched);
            Map<string, bool>::iterator itt = inList.find(node_id);
            bool isInList = (itt != inList.end());
            if (!isInList && attr_matched >= min_attr_match && sat*100 >= matchThreshold) {
                inList.insert(make_pair(node_id, true));
                sortedStringListInsert(matchList, node_id, sat);
                num_matched++;
            }
        }
    }

    for (List<Pair<string, double> >::iterator it = matchList.begin(); it != matchList.end() && (!enable_max_matches || (int) node_ids.size() < max_matches); it++) {
        string node_id = (*it).first;
        node_ids.push_back(node_id);
    }

    return node_ids;
}

/* 
 * Get `max_matches` nodes that are interested in a particular data object. 
 */
NodeRefList
MemoryCache::getNodesForDataObject(
    DataObjectRef &dObj,
    int min_attr_match,
    int max_matches)
{
    string dobj_id = DataObject::idString(dObj);
    List<string> matchListNodes = getNodeIdsForDataObject(dobj_id, min_attr_match, max_matches);
    NodeRefList matched;

    for (List<string>::iterator it = matchListNodes.begin(); it != matchListNodes.end(); it++) {
        string node_id = (*it);
        NodeRef node = getNodeFromId(node_id);
        if (!node) {
            HAGGLE_ERR("Node object is missing from cache: %s\n", node_id.c_str());
            continue;
        }
        matched.add(node);
    }

    return matched;
}

/*
 * Get all the repository entries from the database.
 */
RepositoryEntryList 
MemoryCache::getAllRepository()
{
    RepositoryEntryList repos;

    for (authority_to_repository_t::iterator it = authority_to_repository->begin(); it != authority_to_repository->end(); it++) {
        repos.add((*it).second);
    }

    return repos;
}

/*
 * Insert a repostiory entry into the database. 
 */
int
MemoryCache::insertRepository(
	const RepositoryEntryRef &re)
{
    string auth = re->getAuthority();

    // the logic here is a little funky, but duplicating for backwards
    // compatibilty with existing haggle SQLDataStore
    for (authority_to_repository_t::iterator it = authority_to_repository->find(auth); it != authority_to_repository->end(); it++) {
        if ((*it).first != auth) {
            break;
        }
        const RepositoryEntryRef c_re = (*it).second;
        if (c_re->getKey() != re->getKey()) {
            continue;
        }
       
        if (c_re->getType() != re->getType()) {
            continue;
        }

        authority_to_repository->erase(it);
        break;
    }

    authority_to_repository_max_size++;
    authority_to_repository->insert(make_pair(auth, re));

    return 0;
}

/*
 * Read the repository given an authority and a key.
 */
RepositoryEntryList 
MemoryCache::readRepository(
    string auth,
    string key,
    unsigned int id)
{
    RepositoryEntryList rl;

    if (auth == "") {
        HAGGLE_ERR("Empty authority\n");
        return rl;
    }

    if (id > 0) {
        HAGGLE_ERR("This feature is not currently supported\n");
        errorCount++;
        return rl;
    }

    for (authority_to_repository_t::iterator it = authority_to_repository->find(auth); it != authority_to_repository->end(); it++) {
        if ((*it).first != auth) {
            break;
        }
        const RepositoryEntryRef c_re = (*it).second;

        if (key != "" && key != c_re->getKey()) {
            continue;
        }
        rl.push_back(c_re);
    }

    return rl;
}

/*
 * Delete a repository entry given an authority and key. 
 */
int
MemoryCache::deleteRepository(
    string auth,
    string key,
    unsigned int id)
{
    if (auth == "") {
        HAGGLE_ERR("Empty authority\n");
        return -1;
    }

    if (key == "") {
        HAGGLE_ERR("No key\n");
        return -1;
    }

    if (id > 0) {
        HAGGLE_ERR("This feature is not currently supported\n");
        errorCount++;
        return -1;
    }

    for (authority_to_repository_t::iterator it = authority_to_repository->find(auth); it != authority_to_repository->end(); it++) {
        if ((*it).first != auth) {
            break;
        }
        const RepositoryEntryRef c_re = (*it).second;
        authority_to_repository->erase(it);
    }

    compact();

    return 1;
}

/*
 * Run the unit tests.
 */
bool
MemoryCache::runSelfTests()
{
    bool success = true;

    if (!runSelfTest1()) {
        HAGGLE_ERR("Test 1 FAILED!\n");
        success = false;
    }


    if (!runCompactionTest()) {
        HAGGLE_ERR("Compaction test FAILED\n");
        success = false;
    }

    if (!runMaxMatchesTest()) {
        HAGGLE_ERR("Max matches test FAILED\n");
        success = false;
    }

    if (!runZeroWeightTest()) {
        HAGGLE_ERR("Max matches test FAILED\n");
        success = false;
    }

    HAGGLE_STAT("Summary Statistics - Memory Cache - Unit tests %s\n", success ? "PASSED" : "FAILED");

    return success;
}

/*
 * Return at most `maxToAge` data objects that have no filters or nodes 
 * interested in them and are atleast maxAgeS seconds old.
 */
DataObjectRefList 
MemoryCache::getDataObjectsToAge(int maxAgeS, int maxToAge)
{
    DataObjectRefList dObjs;
    Timeval now = Timeval::now();
    int aged = 0;
    for (do_id_to_do_t::iterator it = do_id_to_do->begin(); (it != do_id_to_do->end()) && (aged < maxToAge); it++) {
        string do_id = (*it).first;
        DataObjectRef dObj = (*it).second->getDataObject();
        // is the data object old enough?
        if ((now - dObj->getReceiveTime()).getTimeAsMilliSeconds() <= maxAgeS * 1000) {
            continue;
        }
        // are there any filters interested?
        if (getFiltersForDataObjectId(do_id).size() > 0) {
            continue;
        }
        // are there any nodes interested?
        if (getNodeIdsForDataObject(do_id).size() > 0) {
            continue;
        }
        aged++;
        dObjs.add(dObj);
    }

    return dObjs;
}

/*
 * Shrink the hash tables that should be shrinked by copying them into
 * a new empty hash table and freeing the original one.
 * 
 * Since the hash tables use the array doubling technique w/ merseene primes,
 * we only create a new hash table when the size since last compation has
 * at least halved (otherwise the table will be the same size).
 */
void
MemoryCache::compact()
{
    // only compact if at least half the DOs were removed
    if (!enableCompaction) {
        return;
    }

    const int compact_factor = 2;
    const int compact_hystersis = 5;

    if (do_id_to_do->size() < ((do_id_to_do_max_size / compact_factor) - compact_hystersis)) {
        do_id_to_do_t *newTable = new do_id_to_do_t();
        compactHashTable(do_id_to_do, newTable);
        do_id_to_do_freed += do_id_to_do_max_size - newTable->size();
        delete do_id_to_do;
        do_id_to_do = newTable;
        do_id_to_do_max_size = do_id_to_do->size();
    }

    if (node_id_to_node->size() < ((node_id_to_node_max_size / compact_factor) - compact_hystersis)) {
        node_id_to_node_t *newTable = new node_id_to_node_t();
        compactHashTable(node_id_to_node, newTable);
        node_id_to_node_freed += node_id_to_node_max_size - newTable->size();
        delete node_id_to_node;
        node_id_to_node = newTable;
        node_id_to_node_max_size = node_id_to_node->size();
    }

    // support for node description data objects (in memory node descriptions
    // must be off)
    if (node_id_to_do_id->size() < ((node_id_to_do_id_max_size / compact_factor) - compact_hystersis)) {
        node_id_to_do_id_t *newTable = new node_id_to_do_id_t();
        compactHashTable(node_id_to_do_id, newTable);
        node_id_to_do_id_freed += node_id_to_do_id_max_size - newTable->size();
        delete node_id_to_do_id;
        node_id_to_do_id = newTable;
        node_id_to_do_id_max_size = node_id_to_do_id->size();
    }

    if (iface_to_node_id->size() < ((iface_to_node_id_max_size / compact_factor) - compact_hystersis)) {
        iface_to_node_id_t *newTable = new iface_to_node_id_t();
        compactHashTable(iface_to_node_id, newTable);
        iface_to_node_id_freed += iface_to_node_id_max_size - newTable->size();
        delete iface_to_node_id;
        iface_to_node_id = newTable;
        iface_to_node_id_max_size = iface_to_node_id->size();
    }

    if (keyval_to_node_id->size() < ((keyval_to_node_id_max_size / compact_factor) - compact_hystersis)) {
        keyval_to_node_id_t *newTable = new keyval_to_node_id_t();
        compactHashTable(keyval_to_node_id, newTable);
        keyval_to_node_id_freed += keyval_to_node_id_max_size - newTable->size();
        delete keyval_to_node_id;
        keyval_to_node_id = newTable;
        keyval_to_node_id_max_size = keyval_to_node_id->size();
    }

    if (key_star_to_node_ids->size() < ((key_star_to_node_ids_max_size / compact_factor) - compact_hystersis)) {
        key_star_to_node_ids_t *newTable = new key_star_to_node_ids_t();
        compactHashTable(key_star_to_node_ids, newTable);
        key_star_to_node_ids_freed += key_star_to_node_ids_max_size - newTable->size();
        delete key_star_to_node_ids;
        key_star_to_node_ids = newTable;
        key_star_to_node_ids_max_size = key_star_to_node_ids->size();
    }

    if (type_to_node_id->size() < ((type_to_node_id_max_size / compact_factor) - compact_hystersis)) {
        type_to_node_id_t *newTable = new type_to_node_id_t();
        compactHashTable(type_to_node_id, newTable);
        type_to_node_id_freed += type_to_node_id_max_size - newTable->size();
        delete type_to_node_id;
        type_to_node_id = newTable;
        type_to_node_id_max_size = type_to_node_id->size();
    }

    if (keyval_to_do_id->size() < ((keyval_to_do_id_max_size / compact_factor) - compact_hystersis)) {
        keyval_to_do_id_t *newTable = new keyval_to_do_id_t();
        compactHashTable(keyval_to_do_id, newTable);
        keyval_to_do_id_freed += keyval_to_do_id_max_size - newTable->size();
        delete keyval_to_do_id;
        keyval_to_do_id = newTable;
        keyval_to_do_id_max_size = keyval_to_do_id->size();
    }

    if (key_star_to_do_ids->size() < ((key_star_to_do_ids_max_size / compact_factor) - compact_hystersis)) {
        key_star_to_do_ids_t *newTable = new key_star_to_do_ids_t();
        compactHashTable(key_star_to_do_ids, newTable);
        key_star_to_do_ids_freed += key_star_to_do_ids_max_size - newTable->size();
        delete key_star_to_do_ids;
        key_star_to_do_ids = newTable;
        key_star_to_do_ids_max_size = key_star_to_do_ids->size();
    }

    if (filters->size() < ((filters_max_size / compact_factor) - compact_hystersis)) {
        filters_t *newTable = new filters_t();
        compactHashTable(filters, newTable);
        filters_freed += filters_max_size - newTable->size();
        delete filters;
        filters = newTable;
        filters_max_size = filters->size();
    }

    if (filter_id_to_attribute->size() < ((filter_id_to_attribute_max_size / compact_factor) - compact_hystersis)) {
        filter_id_to_attribute_t *newTable = new filter_id_to_attribute_t();
        compactHashTable(filter_id_to_attribute, newTable);
        filter_id_to_attribute_freed += filter_id_to_attribute_max_size - newTable->size();
        delete filter_id_to_attribute;
        filter_id_to_attribute = newTable;
        filter_id_to_attribute_max_size = filter_id_to_attribute->size();
    }

    if (keyval_to_filter_id->size() < ((keyval_to_filter_id_max_size / compact_factor) - compact_hystersis)) {
        keyval_to_filter_id_t *newTable = new keyval_to_filter_id_t();
        compactHashTable(keyval_to_filter_id, newTable);
        keyval_to_filter_id_freed += keyval_to_filter_id_max_size - newTable->size();
        delete keyval_to_filter_id;
        keyval_to_filter_id = newTable;
        keyval_to_filter_id_max_size = keyval_to_filter_id->size();
    }

    if (key_star_to_filter_id->size() < ((key_star_to_filter_id_max_size / compact_factor) - compact_hystersis)) {
        key_star_to_filter_id_t *newTable = new key_star_to_filter_id_t();
        compactHashTable(key_star_to_filter_id, newTable);
        key_star_to_filter_id_freed += key_star_to_filter_id_max_size - newTable->size();
        delete key_star_to_filter_id;
        key_star_to_filter_id = newTable;
        key_star_to_filter_id_max_size = key_star_to_filter_id->size();
    }

    if (authority_to_repository->size() < ((authority_to_repository_max_size / compact_factor) - compact_hystersis)) {
        authority_to_repository_t *newTable = new authority_to_repository_t();
        compactHashTable(authority_to_repository, newTable);
        authority_to_repository_freed += authority_to_repository_max_size - newTable->size();
        delete authority_to_repository;
        authority_to_repository = newTable;
        authority_to_repository_max_size = authority_to_repository->size();
    }
}

/*
 * Print out all of the database statistics to the log.
 */
void
MemoryCache::printStats()
{
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of data objects (do_id_to_do): %d\n", (int) do_id_to_do->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of node dos (node_id_to_do_id): %d\n", (int) node_id_to_do_id->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of nodes (node_id_to_node): %d\n", (int) node_id_to_node->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of interfaces (iface_to_node_id): %d\n", (int) iface_to_node_id->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of node attributes (keyval_to_node_id): %d\n", (int) keyval_to_node_id->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of generic node attributes (key_star_to_node_id): %d\n", (int) key_star_to_node_ids->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of types (type_to_node_id): %d\n", (int) type_to_node_id->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of data object attributes (keyval_to_do_id): %d\n", (int) keyval_to_do_id->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of generic data object attributes (keyval_to_do_id): %d\n", (int) key_star_to_do_ids->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of filters (filters): %d\n", (int) filters->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of filter attributes (filter_id_to_attribute): %d\n", (int) filter_id_to_attribute->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of filter attributes (keyval_to_filter_id): %d\n", (int) keyval_to_filter_id->size());
    HAGGLE_STAT("Summary Statistics - Memory Cache - Number of generic filter attributes (keyval_to_filter_id): %d\n", (int) key_star_to_filter_id->size());
}

/*
 * Verify that the zero weight attributes can be included / excluded.
 */
bool
MemoryCache::runZeroWeightTest()
{
    bool pass = true;
    MemoryCache *m = new MemoryCache();
    if (!m) {
        HAGGLE_ERR("Could not allocate memory cache\n");
        return false;
    }

do {

    AbstractNode *n1 = new AbstractNode("n1");
    if (!n1) {
        HAGGLE_ERR("Could not allocate abstract node\n");
        return false;
    }
    n1->addInterface("i1");
    n1->addInterest(createHash("k1", "v1"), 0);
    n1->setMatchThreshold(0);
    n1->setMaxDataObjectsInMatch(1);

    m->cacheNode(n1);

    AbstractDataObject *d1 = new AbstractDataObject("d1");
    d1->_addAttribute(createHash("k1", "v1"));

    m->cacheDataObject(d1);

    m->setExcludeZeroWeightAttributes(false);

    List<string> node_ids = m->getNodeIdsForDataObject(d1->getId());

    if (node_ids.size() != 1) {
        HAGGLE_ERR("Got incorrect number of nodes (no exclude), got: %d != 1\n", (int) node_ids.size());
        pass = false;
        break;
    }

    List<string> dobj_ids = m->getDataObjectIdsForNode(n1->getId());

    if (dobj_ids.size() != 1) {
        HAGGLE_ERR("Got incorrect number of data object ids, got: %d != 1\n", (int) dobj_ids.size());
        pass = false;
        break;
    }

    m->setExcludeZeroWeightAttributes(true);

    node_ids = m->getNodeIdsForDataObject(d1->getId());

    if (node_ids.size() != 0) {
        HAGGLE_ERR("Got incorrect number of nodes (exclude), got: %d != 0\n", (int) node_ids.size());
        pass = false;
        break;
    }

    dobj_ids = m->getDataObjectIdsForNode(n1->getId());

    if (dobj_ids.size() != 0) {
        HAGGLE_ERR("Got incorrect number of data object ids, got: %d != 0\n", (int) dobj_ids.size());
        pass = false;
        break;
    }

} while (false);
    delete m;

    return pass;
}

/*
 * Verify that basic compaction works. 
 */
bool
MemoryCache::runCompactionTest()
{
    bool pass = true;
    MemoryCache *m = new MemoryCache();
    if (!m) {
        HAGGLE_ERR("Could not allocate memory cache\n");
        return false;
    }
    m->setCompaction(true);
    const int size = 20000;
do {
    for (int i = 0; i < size; i++) {
        string id;
        stringprintf(id, "%d", i);
        m->cacheDataObject(new AbstractDataObject(id));
    }

    if (m->getCompactionFreed() != 0) {
        HAGGLE_ERR("No compaction should have been triggered, but had: %d.\n", (int) m->getCompactionFreed());
        pass = false;
        break;
    }

    for (int i = 0; i < size; i++) {
        string id;
        stringprintf(id, "%d", i);
        m->unCacheDataObject(id);
    }

    if (m->getCompactionFreed() <= 0) {
        HAGGLE_ERR("No compaction was triggered.\n");
        pass = false;
        break;
    }
} while (false);
    delete m;

    return pass;
}

/*
 * Test to verify that maximum matches is respected, and
 * matches are ordered by degree of satisfaction.
 */
bool
MemoryCache::runMaxMatchesTest()
{
    bool pass = true;
    MemoryCache *m = new MemoryCache();
    if (!m) {
        HAGGLE_ERR("Could not allocate memory cache\n");
        return false;
    }
    m->setMaxNodesToMatch(1);
do {

    AbstractNode *n1 = new AbstractNode("n1");
    if (!n1) {
        HAGGLE_ERR("Could not allocate abstract node\n");
        return false;
    }
    n1->addInterface("i1");
    n1->addInterest(createHash("k1", "v1"), 1);
    n1->addInterest(createHash("k2", "v2"), 1);
    n1->setMatchThreshold(0);
    n1->setMaxDataObjectsInMatch(1);

    m->cacheNode(n1);

    AbstractNode *n2 = new AbstractNode("n2");
    if (!n2) {
        HAGGLE_ERR("Could not allocate abstract node\n");
        return false;
    }
    n2->addInterface("i1");
    n2->addInterest(createHash("k1", "v1"), 1);
    n2->setMatchThreshold(0);
    n2->setMaxDataObjectsInMatch(1);

    m->cacheNode(n2);

    AbstractNode *n3 = new AbstractNode("n3");
    if (!n3) {
        HAGGLE_ERR("Could not allocate abstract node\n");
        return false;
    }
    n3->addInterface("i1");
    n3->addInterest(createHash("k1", "v1"), 1);
    n3->addInterest(createHash("k2", "v2"), 1);
    n3->setMatchThreshold(0);
    n3->setMaxDataObjectsInMatch(1);

    m->cacheNode(n3);

    AbstractDataObject *d1 = new AbstractDataObject("d1");
    d1->_addAttribute(createHash("k1", "v1"));

    m->cacheDataObject(d1);

    List<string> node_ids = m->getNodeIdsForDataObject(d1->getId());

    if (node_ids.size() != 1) {
        HAGGLE_ERR("Got incorrect number of nodes, got: %d != 1\n", (int) node_ids.size());
        pass = false;
        break;
    }

    string node_id = *(node_ids.begin());
    if (node_id != n2->getId()) {
        HAGGLE_ERR("Did not sort node matches by degree of satisfaction.\n");
        pass = false;
        break;
    }

    AbstractDataObject *d2 = new AbstractDataObject("d2");
    d2->_addAttribute(createHash("k1", "v1"));
    d2->_addAttribute(createHash("k2", "v2"));

    m->cacheDataObject(d2);

    AbstractDataObject *d3 = new AbstractDataObject("d3");
    d3->_addAttribute(createHash("k3", "v3"));

    List<string> do_ids = m->getDataObjectIdsForNode(n3->getId());


    if (do_ids.size() != 1) {
        HAGGLE_ERR("Got incorrect number of data objects, got: %d != 1\n", (int) do_ids.size());
        pass = false;
        break;
    }

    string do_id = *(do_ids.begin());

    if (do_id != d2->getId()) {
        HAGGLE_ERR("Did not sort do matches by degree of satisfaction, got do: %s\n", do_id.c_str());
        pass = false;
        break;
    }

} while (false);

    return pass;
}

/*
 * Run the first unit test, goes through most the basic functionality of 
 * inserting nodes, data objects, filters, and repository entries and 
 * confirms nothing is amiss.  
 */
bool
MemoryCache::runSelfTest1()
{
    bool pass = true;
    MemoryCache *m = new MemoryCache();
    if (!m) {
        HAGGLE_ERR("Could not allocate memory cache\n");
        return false;
    }
do {
    
    AbstractDataObject *d1 = new AbstractDataObject("d1");
    d1->_addAttribute(createHash("k1", "v1"));
    AbstractDataObject *d2 = new AbstractDataObject("d2");
    d2->_addAttribute(createHash("k2", "v2"));

    m->cacheDataObject(d1);
    m->cacheDataObject(d2);

    AbstractNode *n1 = new AbstractNode("n1");
    n1->addInterface("i1");
    n1->addInterest(createHash("k1", "v1"), 1);
    n1->addInterest(createHash("k2", "v2"), 0);
    n1->setMatchThreshold(100);
    n1->setMaxDataObjectsInMatch(1);

    m->cacheNode(n1);

    List<string> matches = m->getDataObjectIdsForNode(n1->getId());
    int count = 0;
    string found_id = "";
    for (List<string>::iterator it = matches.begin(); it != matches.end(); it++) {
        found_id = (*it);
        count++;
    }

    if (count != 1) {
        HAGGLE_ERR("T1: Retrieved the wrong number of data objects for node: %s, got: %d != expected %d\n", n1->getId().c_str(), count, 1);
        pass = false;
        break;
    }

    if (found_id != d1->getId()) {
        HAGGLE_ERR("T2: Got wrong data object id: %s\n", found_id.c_str());
        pass = false;
        break;
    }

    matches = m->getNodeIdsForDataObject(d1->getId());
    count = 0;
    found_id = "";
    
    for (List<string>::iterator it = matches.begin(); it != matches.end(); it++) {
        found_id = (*it);
        count++;
    }

    if (count != 1) {
        HAGGLE_ERR("T3: Retrieved the wrong number of nodes for data object: %s, got: %d != expected %d\n", d1->getId().c_str(), count, 1);
        pass = false;
        break;
    }

    if (found_id != n1->getId()) {
        HAGGLE_ERR("T4: Got wrong node id: %s\n", found_id.c_str());
        pass = false;
        break;
    }

    int filter_id  = m->getTemporaryFilterId();
    m->cacheFilter(filter_id, "k1", "v1", 1);
    m->cacheFilter(filter_id, "k2", "v2", 1);
    m->setFilterSettings(filter_id, 50);

    matches = m->getDataObjectIdsForFilter(filter_id);

    count = 0;
    for (List<string>::iterator it = matches.begin(); it != matches.end(); it++) {
        count++;
    }

    if (count != 2) {
        HAGGLE_ERR("T5: Retrieved the wrong number of data objects for filter: %d, got: %d != expected %d\n", filter_id, count, 2);
        pass = false;
        break;
    }

    count = 0;
    List<long> matched_filters = m->getFiltersForDataObjectId(d1->getId());
    for (List<long>::iterator it = matched_filters.begin(); it != matched_filters.end(); it++) {
        count++;
    }

    if (count != 1) {
        HAGGLE_ERR("T6: Retrieved the wrong number of filters for data object: %s, got: %d != expected %d\n", d1->getId().c_str(), count, 1);
        pass = false;
        break;
    }

    count = 0;
    matched_filters = m->getFiltersForDataObjectId(d2->getId());
    for (List<long>::iterator it = matched_filters.begin(); it != matched_filters.end(); it++) {
        count++;
    }

    if (count != 1) {
        HAGGLE_ERR("T7: Retrieved the wrong number of filters for data object: %s, got: %d != expected %d\n", d2->getId().c_str(), count, 1);
        pass = false;
        break;
    }

    // test filter asteriks
    int filter_id2  = m->getTemporaryFilterId();
    m->cacheFilter(filter_id2, "k1", "*", 1);
    m->setFilterSettings(filter_id2, 100);

    matches = m->getDataObjectIdsForFilter(filter_id2);

    count = 0;
    for (List<string>::iterator it = matches.begin(); it != matches.end(); it++) {
        count++;
    }

    if (count != 1) {
        HAGGLE_ERR("T7a: Retrieved the wrong number of data objects for filter: %d, got: %d != expected %d\n", filter_id2, count, 1);
        pass = false;
        break;
    }

    count = 0;
    matched_filters = m->getFiltersForDataObjectId(d1->getId());
    for (List<long>::iterator it = matched_filters.begin(); it != matched_filters.end(); it++) {
        count++;
    }

    if (count != 2) {
        HAGGLE_ERR("T7b: Retrieved the wrong number of filters for data object: %s, got: %d != expected %d\n", d1->getId().c_str(), count, 2);
        pass = false;
        break;
    }

    m->unCacheFilter(filter_id2);

    // done with filter asteriks test

    AbstractDataObject *d3 = new AbstractDataObject("d3");
    d3->_addAttribute(createHash("k1", "v1"));
    d3->_addAttribute(createHash("k2", "v2"));
    d3->_addAttribute(createHash("k3", "v3"));

    m->cacheDataObject(d3);

    matches = m->getDataObjectIdsForFilter(filter_id);

    count = 0;
    for (List<string>::iterator it = matches.begin(); it != matches.end(); it++) {
        count++;
    }

    if (count != 3) {
        HAGGLE_ERR("T8: Retrieved the wrong number of data objects for filter: %d, got: %d != expected %d\n", filter_id, count, 3);
        pass = false;
        break;
    }

    AbstractNode *n2 = new AbstractNode("n2");
    n2->addInterface("i2");
    n2->addInterest(createHash("k1", "v1"), 1);
    n2->addInterest(createHash("k2", "v2"), 1);
    n2->addInterest(createHash("k3", "v3"), 1);
    n2->setMatchThreshold(100);
    n2->setMaxDataObjectsInMatch(1);

    m->cacheNode(n2);

    matches = m->getNodeIdsForDataObject(d3->getId());
    count = 0;
    
    for (List<string>::iterator it = matches.begin(); it != matches.end(); it++) {
        found_id = (*it);
        count++;
    }

    if (count != 2) {
        HAGGLE_ERR("T9: Retrieved the wrong number of nodes for data object: %s, got: %d != expected %d\n", d3->getId().c_str(), count, 2);
        pass = false;
        break;
    }

    m->unCacheDataObject(d3->getId());

    matches = m->getDataObjectIdsForFilter(filter_id);

    count = 0;
    for (List<string>::iterator it = matches.begin(); it != matches.end(); it++) {
        count++;
    }

    if (count != 2) {
        HAGGLE_ERR("T10: Retrieved the wrong number of data objects for filter: %d, got: %d != expected %d\n", filter_id, count, 2);
        pass = false;
        break;
    }

    m->unCacheFilter(filter_id);

    m->unCacheNode(n1->getId());
   
    matches = m->getNodeIdsForDataObject(d1->getId());
    
    count = 0;
    for (List<string>::iterator it = matches.begin(); it != matches.end(); it++) {
        found_id = (*it);
        count++;
    }

    if (count != 0) {
        HAGGLE_ERR("T11: Retrieved the wrong number of nodes for data object: %s, got: %d != expected %d\n", d1->getId().c_str(), count, 0);
        pass = false;
        break;
    }

    m->unCacheNode(n2->getId());

    m->unCacheDataObject(d2->getId());

    m->unCacheDataObject(d1->getId());

    // test repository basics

    string auth1 = "auth1";
    string key1 = "privKey1";
    string key2 = "privKey2";
    RepositoryEntryRef rep1 = new RepositoryEntry(auth1, key1, "stuff1");
    RepositoryEntryRef rep2 = new RepositoryEntry(auth1, key2, "stuff2");

    m->insertRepository(rep1);
    m->insertRepository(rep2);

    RepositoryEntryList reps = m->readRepository(auth1);

    count = 0;
    for (RepositoryEntryList::iterator it = reps.begin(); it != reps.end(); it++) {
        RepositoryEntryRef rep = (*it);
        count++;
    }

    if (count != 2) {
        HAGGLE_ERR("T12: Did not receive proper amount of repository entries in search 1\n");
        pass = false;
    }
  
    reps = m->readRepository(auth1, key1);
    count = 0;
    for (RepositoryEntryList::iterator it = reps.begin(); it != reps.end(); it++) {
        RepositoryEntryRef rep = (*it);
        count++;
    }

    if (count != 1) {
        HAGGLE_ERR("T13: Did not receive proper amount of repository entries in search 2\n");
        pass = false;
    }

    m->deleteRepository(auth1, key1);
    m->deleteRepository(auth1, key2);

    if (m->getErrorCount() > 0) {
        HAGGLE_ERR("Error occurred\n");
        pass = false;
        break;
    }

}
while (false);

    delete m;

    return pass;
}

